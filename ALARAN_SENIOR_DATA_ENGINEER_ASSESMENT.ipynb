{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1.INSTALL NECESSARY LIBRARIES\n"
      ],
      "metadata": {
        "id": "C-Y0C95YlBYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuLkJqMQOXT9",
        "outputId": "273940c3-62f3-4962-994d-3de8aebc65af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.26.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.27.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "Requirement already satisfied: apscheduler in /usr/local/lib/python3.10/dist-packages (3.10.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from apscheduler) (1.16.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from apscheduler) (2024.2)\n",
            "Requirement already satisfied: tzlocal!=3.*,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apscheduler) (5.2)\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "! sudo pip install pyspark\n",
        "! sudo pip install polars\n",
        "! pip install selenium\n",
        "! sudo apt-get update -y\n",
        "! sudo apt install chromium-chromedriver -y\n",
        "! sudo pip install apscheduler\n",
        "\n",
        "! sudo cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.IMPORT LIBRARIES\n"
      ],
      "metadata": {
        "id": "-YofSmUMlLd2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GBVio33T2LKf"
      },
      "outputs": [],
      "source": [
        "# Import regex for text parsing, sys for path specification and time for delaying execution\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "\n",
        "# import selenium sub-modules and functions for web scraping\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support.ui import Select\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "\n",
        "# import pyspark functions\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import *\n",
        "\n",
        "import random\n",
        "from datetime import date, timedelta\n",
        "\n",
        "# Import polars for automatic schema infering (suitable for big data, unlike pandas.)\n",
        "# has similar syntax to pyspark\n",
        "import polars as pl\n",
        "\n",
        "# Import logging to monitor data pipelines errors, info, debug and warnings\n",
        "import logging\n",
        "# Import apscheduler to schedule pyspark jobs in Notebooks\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "## Note: Although ETL Data pipeline is easier and faster in Notebooks,\n",
        "## a proper scheduling tool like Airflow is recommended.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.SETUP CONFIGURATION (Selenium and Pyspark)\n"
      ],
      "metadata": {
        "id": "vy9NCShmlS_O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XtoNeGN1Pfhi"
      },
      "outputs": [],
      "source": [
        "# Selenium configuration\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "chrome_options.binary_location = '/usr/bin/chromium-browser'\n",
        "chrome_options.add_argument(\"start-maximized\")\n",
        "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "chrome_options.add_experimental_option('useAutomationExtension', False)\n",
        "wd = webdriver.Chrome(options=chrome_options )\n",
        "\n",
        "# Spark configuration\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "                    .appName('test') \\\n",
        "                    .enableHiveSupport() \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "# Loggings configuration\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK A: MONTH-OVER-MONTH (MoM) GROWTH RATE OF REVENUE"
      ],
      "metadata": {
        "id": "qJGZv-MVDaT_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwkHk47VPeoY"
      },
      "source": [
        "##i. Extract Data\n",
        "(Web scrape Schema queries and tables with selenium)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QwEE4eNP4na",
        "outputId": "065552c6-04d2-483a-8c4b-97d380811afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    CREATE TABLE Sales (\n",
            "        sale_id INT ,\n",
            "        sale_date DATE,\n",
            "        revenue DECIMAL(10, 2)\n",
            "    );\n"
          ]
        }
      ],
      "source": [
        "# Chrome driver to visit clickup site to scrape queries and table\n",
        "wd = webdriver.Chrome(options=chrome_options )\n",
        "wd.get('https://doc.clickup.com/3627772/p/h/3epqw-172805/3c092c33c63dada')\n",
        "# Wait for all page elements to load\n",
        "time.sleep(5)\n",
        "# FInd the particular schema in web page\n",
        "query1 = wd.find_elements(By.XPATH, \"/html/body/app-root/cu-document-page/cu-dashboard-doc-container/div/div[2]/div/div/cu-dashboard-doc-main/div/div/div/cu-document-page-content/div/div/div/div/pre[1]\")\n",
        "extracted_query = (' '.join([element.text for element in query1])).replace('PRIMARY KEY', \"\")\n",
        "print(extracted_query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ii. Clean and parse data from schema"
      ],
      "metadata": {
        "id": "UkhdPj0rRUFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse Table name from schema\n",
        "pattern1 = r\"CREATE\\s+TABLE\\s+(\\w+)\"\n",
        "table_name = re.search(pattern1, extracted_query)\n",
        "table_name = table_name.group(1)\n",
        "print(table_name)\n",
        "# Parse Columns from Schema\n",
        "pattern2 = r'\\b([a-z]\\w+)\\b'\n",
        "matches = re.findall(pattern2, extracted_query, re.MULTILINE)\n",
        "column_names = [col for col in matches]\n",
        "print(column_names)\n",
        "\n",
        "# Generate random data\n",
        "num_rows = 100\n",
        "data = []\n",
        "for _ in range(num_rows):\n",
        "  sale_id = random.randint(1, 1000)\n",
        "  sale_date = date.today() - timedelta(days=random.randint(0, 365))\n",
        "  revenue = random.uniform(100, 1000)\n",
        "  data.append((sale_id, sale_date, revenue))\n",
        "# Create Spark Dataframe from random Data\n",
        "sales_df = pl.DataFrame({'sale_id': [row[0] for row in data],\n",
        "                         'sale_date': [row[1] for row in data],\n",
        "                         'revenue': [row[2] for row in data]\n",
        "                         })\n",
        "\n",
        "sales_df = spark.createDataFrame(sales_df.to_pandas(), column_names)\n",
        "sales_df.show()\n",
        "# Convert spark dataframe to SQL table\n",
        "sales_df.createOrReplaceTempView(table_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOCsJRWbnr2-",
        "outputId": "cb3f93fa-6c77-4991-ec8c-97657107490f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sales\n",
            "['sale_id', 'sale_date', 'revenue']\n",
            "+-------+-------------------+------------------+\n",
            "|sale_id|          sale_date|           revenue|\n",
            "+-------+-------------------+------------------+\n",
            "|    886|2024-06-24 00:00:00| 960.8227733359782|\n",
            "|    189|2024-05-05 00:00:00|444.08128089647465|\n",
            "|     66|2023-11-23 00:00:00|  534.991458234937|\n",
            "|    854|2024-09-04 00:00:00| 982.8353666217429|\n",
            "|    353|2024-04-20 00:00:00| 835.5696439984451|\n",
            "|    910|2024-07-15 00:00:00| 952.3920479357321|\n",
            "|    742|2024-05-17 00:00:00| 737.5079645460632|\n",
            "|    518|2024-08-08 00:00:00| 710.3577563934969|\n",
            "|    130|2024-04-11 00:00:00|466.19366746945224|\n",
            "|    841|2024-07-21 00:00:00|196.80438299879847|\n",
            "|    501|2024-10-20 00:00:00| 736.0120205536888|\n",
            "|    176|2023-12-15 00:00:00| 610.1055704569328|\n",
            "|    589|2024-07-07 00:00:00| 573.1140479317834|\n",
            "|    706|2024-06-14 00:00:00| 918.3445102735355|\n",
            "|    990|2023-12-12 00:00:00| 923.5539593056792|\n",
            "|     21|2024-01-05 00:00:00| 469.5571565300676|\n",
            "|     20|2024-04-03 00:00:00|225.15080488880074|\n",
            "|    398|2024-09-12 00:00:00| 919.2983464991871|\n",
            "|    487|2024-04-28 00:00:00|217.80880082608004|\n",
            "|    367|2024-10-31 00:00:00| 785.2631626758017|\n",
            "+-------+-------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iii. Calculate the month-over-month (MoM) growth rate of revenue\n",
        "\n"
      ],
      "metadata": {
        "id": "wMuLGRQfV_T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write SQL queries\n",
        "## SQL query to extract month from datetime and aggregate by month\n",
        "sales_monthly_revenue = spark.sql(''' SELECT\n",
        "                                           DATE( DATE_TRUNC('month', sale_date)) AS month,\n",
        "                                            SUM(revenue) AS total_revenue\n",
        "                                      FROM\n",
        "                                          Sales\n",
        "                                      GROUP BY\n",
        "                                          DATE_TRUNC('month', sale_date)\n",
        "                                  ''')\n",
        "### save df to sql table\n",
        "sales_monthly_revenue.createOrReplaceTempView('Sales_monthly_revenue')\n",
        "\n",
        "## Extract Previous month revenue from the sales_monthly_revenue\n",
        "sales_previous_month_revenue = spark.sql('''\n",
        "                                            SELECT\n",
        "                                                month,\n",
        "                                                total_revenue,\n",
        "                                                LAG(total_revenue, 1) OVER (ORDER BY month) AS previous_month_revenue\n",
        "                                            FROM\n",
        "                                                Sales_monthly_revenue;''')\n",
        "### save df to sql table\n",
        "sales_previous_month_revenue.createOrReplaceTempView('Sales_previous_month_revenue')\n",
        "\n",
        "## Extract growth rate from sales_previous_month_revenue\n",
        "sales_growth_rate = spark.sql('''\n",
        "                                SELECT\n",
        "                                    month,\n",
        "                                    total_revenue,\n",
        "                                    previous_month_revenue,\n",
        "                                    (total_revenue - previous_month_revenue) / previous_month_revenue * 100 AS growth_rate\n",
        "                                FROM\n",
        "                                    Sales_previous_month_revenue;''')\n",
        "sales_growth_rate.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcOlLRdZWKjO",
        "outputId": "3b381733-f7a8-4a9d-d111-6c5990b661dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+----------------------+-------------------+\n",
            "|     month|     total_revenue|previous_month_revenue|        growth_rate|\n",
            "+----------+------------------+----------------------+-------------------+\n",
            "|2023-11-01|1219.5695387826572|                  NULL|               NULL|\n",
            "|2023-12-01| 5905.270696372385|    1219.5695387826572|  384.2094287027596|\n",
            "|2024-01-01| 3502.524719683244|     5905.270696372385| -40.68815978520936|\n",
            "|2024-02-01| 4588.023529640132|     3502.524719683244| 30.991895756129235|\n",
            "|2024-03-01|  1561.78862927706|     4588.023529640132| -65.95944595341776|\n",
            "|2024-04-01|  6500.80446059534|      1561.78862927706|  316.2409905368893|\n",
            "|2024-05-01| 4854.591571684812|      6500.80446059534|-25.323218055381542|\n",
            "|2024-06-01|6550.3153001863975|     4854.591571684812| 34.930306771679156|\n",
            "|2024-07-01| 6407.765337055567|    6550.3153001863975|-2.1762305568218046|\n",
            "|2024-08-01| 8006.938173879222|     6407.765337055567|  24.95679465001274|\n",
            "|2024-09-01| 4134.839653836544|     8006.938173879222| -48.35929085445546|\n",
            "|2024-10-01| 6576.121491381198|     4134.839653836544| 59.041753536427734|\n",
            "|2024-11-01|1043.3809695096243|     6576.121491381198| -84.13379419955818|\n",
            "+----------+------------------+----------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Or write the SQL in a single query"
      ],
      "metadata": {
        "id": "zMExEjxrx4pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Write SQL queries\n",
        "sales_df = spark.sql('''WITH Revenue_Per_Month AS (\n",
        "    SELECT\n",
        "        DATE(DATE_TRUNC('month', sale_date)) AS month_only,\n",
        "        SUM(revenue) AS revenue_sum\n",
        "    FROM\n",
        "        Sales\n",
        "    GROUP BY\n",
        "        DATE_TRUNC('month', sale_date)\n",
        "),\n",
        "Revenue_previous AS (\n",
        "    SELECT\n",
        "        month_only,\n",
        "        revenue_sum,\n",
        "        LAG(revenue_sum, 1) OVER (ORDER BY month_only) AS previous_month_revenue\n",
        "    FROM\n",
        "        Revenue_Per_Month\n",
        ")\n",
        "SELECT\n",
        "    month_only,\n",
        "    revenue_sum,\n",
        "    previous_month_revenue,\n",
        "    (revenue_sum - previous_month_revenue) / previous_month_revenue * 100 AS growth_rate\n",
        "FROM\n",
        "    Revenue_previous;;''')\n",
        "#sales_df = sales_df.drop\n",
        "sales_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZFrpf0ZyA2j",
        "outputId": "56cdf07c-1b51-4cd8-b350-d3365c4337da"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+----------------------+-------------------+\n",
            "|month_only|       revenue_sum|previous_month_revenue|        growth_rate|\n",
            "+----------+------------------+----------------------+-------------------+\n",
            "|2023-11-01|1219.5695387826572|                  NULL|               NULL|\n",
            "|2023-12-01| 5905.270696372385|    1219.5695387826572|  384.2094287027596|\n",
            "|2024-01-01| 3502.524719683244|     5905.270696372385| -40.68815978520936|\n",
            "|2024-02-01| 4588.023529640132|     3502.524719683244| 30.991895756129235|\n",
            "|2024-03-01|  1561.78862927706|     4588.023529640132| -65.95944595341776|\n",
            "|2024-04-01|  6500.80446059534|      1561.78862927706|  316.2409905368893|\n",
            "|2024-05-01| 4854.591571684812|      6500.80446059534|-25.323218055381542|\n",
            "|2024-06-01|6550.3153001863975|     4854.591571684812| 34.930306771679156|\n",
            "|2024-07-01| 6407.765337055567|    6550.3153001863975|-2.1762305568218046|\n",
            "|2024-08-01| 8006.938173879222|     6407.765337055567|  24.95679465001274|\n",
            "|2024-09-01| 4134.839653836544|     8006.938173879222| -48.35929085445546|\n",
            "|2024-10-01| 6576.121491381198|     4134.839653836544| 59.041753536427734|\n",
            "|2024-11-01|1043.3809695096243|     6576.121491381198| -84.13379419955818|\n",
            "+----------+------------------+----------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iv. Load data to csv format"
      ],
      "metadata": {
        "id": "Utt5AjXsaHm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sales_df.write.csv(\"RevenueGrowthRate(MOM).csv\", mode='Overwrite')\n"
      ],
      "metadata": {
        "id": "rTB6R_wFrDOg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASK B: DAILY UTILIZATION OF EACH VEHICLE"
      ],
      "metadata": {
        "id": "ik4cbodlyf0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##i. Extract Data"
      ],
      "metadata": {
        "id": "592KJE6ta9kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Chrome driver to visit clickup site to scrape queries and table\n",
        "extract = wd.get('https://doc.clickup.com/3627772/p/h/3epqw-172805/3c092c33c63dada')\n",
        "# Wait for all page elements to load\n",
        "time.sleep(5)\n",
        "# Find specific tables in the web page\n",
        "rows = wd.find_elements(By.XPATH, \"//table[@class='clickup-table']/tbody/tr\")\n",
        "table_data = []\n",
        "for row in rows:\n",
        "  cols = row.find_elements(By.TAG_NAME, \"td\")\n",
        "  row_data = [col.text for col in cols]\n",
        "  table_data.append(row_data)\n"
      ],
      "metadata": {
        "id": "3T9X2jqEyNqb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Vehicles columns and data\n",
        "table_1 = table_data[0][0]\n",
        "table_1_cols = table_data[1]\n",
        "table_1_data = table_data[2]\n",
        "table_1_df = pl.DataFrame({str(table_1_cols[0]): [table_1_data[0]],\n",
        "                           str(table_1_cols[1]): [table_1_data[1]],\n",
        "                           str(table_1_cols[2]): [table_1_data[2]],\n",
        "                           str(table_1_cols[3]): [table_1_data[3]],\n",
        "                           str(table_1_cols[4]): [table_1_data[4]],\n",
        "                           str(table_1_cols[5]): [table_1_data[5]],\n",
        "                           str(table_1_cols[6]): [table_1_data[6]],\n",
        "                           str(table_1_cols[7]): [table_1_data[7]]\n",
        "                           })\n",
        "table_1_df = spark.createDataFrame(table_1_df.to_pandas(), table_1_cols)\n",
        "vehicles_df = table_1_df\n",
        "vehicles_df.show()\n",
        "\n",
        "# Extract Vehicle History columns and data\n",
        "table_2 = table_data[3][0]\n",
        "table_2_cols = table_data[4]\n",
        "table_2_data = table_data[5:14]\n",
        "table_2_df = pl.DataFrame({\n",
        "    str(table_2_cols[0]): [row[0] for row in table_2_data],\n",
        "    str(table_2_cols[1]): [row[1] for row in table_2_data],\n",
        "    str(table_2_cols[2]): [row[2] for row in table_2_data],\n",
        "    str(table_2_cols[3]): [row[3] for row in table_2_data],\n",
        "    str(table_2_cols[4]): [row[4] for row in table_2_data],\n",
        "    str(table_2_cols[5]): [row[5] for row in table_2_data],\n",
        "    str(table_2_cols[6]): [row[6] for row in table_2_data],\n",
        "    str(table_2_cols[7]): [row[7] for row in table_2_data],\n",
        "    str(table_2_cols[8]): [row[8] for row in table_2_data]\n",
        "})\n",
        "table_2_df = spark.createDataFrame(table_2_df.to_pandas())\n",
        "vehicles_history_df = table_2_df\n",
        "vehicles_history_df.show()\n",
        "\n",
        "# Extract location table data and columns\n",
        "table_3 = table_data[14][0]\n",
        "table_3_cols = table_data[15]\n",
        "table_3_data = table_data[16]\n",
        "table_3_df = pl.DataFrame({\n",
        "    str(table_3_cols[0]): [table_3_data[0]],\n",
        "    str(table_3_cols[1]): [table_3_data[1]],\n",
        "    str(table_3_cols[2]): [table_3_data[2]],\n",
        "    str(table_3_cols[3]): [table_3_data[3]],\n",
        "    str(table_3_cols[4]): [table_3_data[4]],\n",
        "    str(table_3_cols[5]): [table_3_data[5]],\n",
        "    str(table_3_cols[6]): [table_3_data[6]]\n",
        "     })\n",
        "table_3_df = table_3_df.to_pandas()\n",
        "table_3_df = spark.createDataFrame(table_3_df)\n",
        "location_df = table_3_df\n",
        "location_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFkNJ_7QHHmU",
        "outputId": "b99dc20a-a0f5-4884-cb11-246532806527"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+--------------------+------+-----------+-------------------+-------------------+----------+\n",
            "|vehicle_id |vehicle_name|license_plate_number|status|location_id|         created_at|         updated_at|deleted_at|\n",
            "+-----------+------------+--------------------+------+-----------+-------------------+-------------------+----------+\n",
            "|          2|Ford Everest|              1Q9243|  YARD|          4|2022-09-04 23:37:56|2022-11-15 00:00:16|          |\n",
            "+-----------+------------+--------------------+------+-----------+-------------------+-------------------+----------+\n",
            "\n",
            "+-----+----------+-----------+----------+-------------------+-------------------+----------+-------------------+-------------------+\n",
            "|   id|vehicle_id|location_id|    status|         created_at|         updated_at|deleted_at|         start_time|           end_time|\n",
            "+-----+----------+-----------+----------+-------------------+-------------------+----------+-------------------+-------------------+\n",
            "|10618|         2|          4|    ONRENT|2022-11-14 00:00:13|2022-11-14 11:30:12|          |2022-11-14 00:00:13|2022-11-14 11:30:12|\n",
            "|10685|         2|          4|   OVERDUE|2022-11-14 11:30:12|2022-11-14 11:49:47|          |2022-11-14 11:30:12|2022-11-14 11:49:47|\n",
            "|10690|         2|          4|      YARD|2022-11-14 11:49:47|2022-11-14 11:49:47|          |2022-11-14 11:49:47|2022-11-14 11:49:47|\n",
            "|10691|         2|          4|   PENDING|2022-11-14 11:49:47|2022-11-14 13:30:38|          |2022-11-14 11:49:47|2022-11-14 13:30:38|\n",
            "|10705|         2|          4|      YARD|2022-11-14 13:30:38|2022-11-14 13:31:49|          |2022-11-14 13:30:38|2022-11-14 13:31:49|\n",
            "|10706|         2|          4|RELOCATION|2022-11-14 13:31:49|2022-11-14 13:32:34|          |2022-11-14 13:31:49|2022-11-14 13:32:34|\n",
            "|10707|         2|          4|      YARD|2022-11-14 13:32:34|2022-11-14 13:32:34|          |2022-11-14 13:32:34|2022-11-14 13:32:34|\n",
            "|10708|         2|          4|   PENDING|2022-11-14 13:32:34|2022-11-14 13:33:36|          |2022-11-14 13:32:34|2022-11-14 13:33:36|\n",
            "|10709|         2|          4|      YARD|2022-11-14 13:33:36|2022-11-15 00:00:16|          |2022-11-14 13:33:36|2022-11-15 00:00:16|\n",
            "+-----+----------+-----------+----------+-------------------+-------------------+----------+-------------------+-------------------+\n",
            "\n",
            "+-----------+----------------+--------+---------+-------------------+-------------------+----------+\n",
            "|location_id|   location_name|latitude|longitude|         created_at|         updated_at|deleted_at|\n",
            "+-----------+----------------+--------+---------+-------------------+-------------------+----------+\n",
            "|          4|Mandalay Airport|21.70487|95.968993|2022-09-04 20:37:56|2022-09-04 20:37:56|          |\n",
            "+-----------+----------------+--------+---------+-------------------+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ii. Clean and Join tables"
      ],
      "metadata": {
        "id": "NkmkW5SDAWdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any leading/trailing spaces from column names in both DataFrames\n",
        "vehicles_df = vehicles_df.toDF(*[c.strip() for c in vehicles_df.columns])\n",
        "vehicles_history_df = vehicles_history_df.toDF(*[c.strip() for c in vehicles_history_df.columns])\n",
        "location_df = location_df.toDF(*[c.strip() for c in location_df.columns])\n",
        "\n",
        "## Rename Vehicle columns\n",
        "vehicles_df = vehicles_df.withColumnRenamed(\"status\", \"status(vehicle)\") \\\n",
        "                         .withColumnRenamed(\"created_at\", \"created_at(vehicle)\") \\\n",
        "                          .withColumnRenamed(\"updated_at\", \"updated_at(vehicle)\") \\\n",
        "                          .withColumnRenamed(\"deleted_at\", \"deleted_at(vehicle)\") \\\n",
        "                          .withColumnRenamed(\"location_id\", \"location_id(vehicle)\")\n",
        "# Rename Vehicle History columns\n",
        "vehicles_history_df = vehicles_history_df.withColumnRenamed(\"created_at\", \"created_at(vehicle_history)\") \\\n",
        "                                          .withColumnRenamed(\"updated_at\", \"updated_at(vehicle_history)\")\\\n",
        "                                          .withColumnRenamed(\"deleted_at\", \"deleted_at(vehicle_history)\")\\\n",
        "                                          .withColumnRenamed(\"status\", \"status(vehicle_history)\")\\\n",
        "                                          .withColumnRenamed(\"start_time\", \"start_time(vehicle_history)\")\\\n",
        "                                          .withColumnRenamed(\"end_time\", \"end_time(vehicle_history)\")\\\n",
        "                                          .withColumnRenamed(\"id\", \"history_id\")\n",
        "# Rename Location Columns\n",
        "location_df = location_df.withColumnRenamed(\"created_at\", \"created_at(location)\")\\\n",
        "                          .withColumnRenamed(\"updated_at\", \"updated_at(location)\")\\\n",
        "                          .withColumnRenamed(\"deleted_at\", \"deleted_at(location)\")\\\n",
        "\n",
        "\n",
        "# Right join vehicles and vehicle_history tables\n",
        "vehicles_joined = vehicles_df.join(vehicles_history_df, on='vehicle_id', how=\"right\")\n",
        "# Left join vehicles_joined and location tables\n",
        "vehicles_location = vehicles_joined.join(location_df, on='location_id', how=\"left\")\n",
        "\n",
        "# Change columns data type\n",
        "column_type_mapping = {\n",
        "    \"vehicle_id\": IntegerType(),\n",
        "    \"location_id\": IntegerType(),\n",
        "    \"latitude\": DoubleType(),\n",
        "    \"longitude\": DoubleType(),\n",
        "    \"vehicle_name\": StringType(),\n",
        "    \"license_plate_number\": StringType(),\n",
        "    \"status(vehicle)\": StringType(),\n",
        "    \"created_at(vehicle)\": TimestampType(),\n",
        "    \"updated_at(vehicle)\": TimestampType(),\n",
        "    \"deleted_at(vehicle)\": TimestampType(),\n",
        "    \"history_id\": IntegerType(),\n",
        "    \"vehicle_id\": IntegerType(),\n",
        "    \"location_id(vehicle)\": IntegerType(),\n",
        "    \"start_time(vehicle_history)\": TimestampType(),\n",
        "    \"end_time(vehicle_history)\": TimestampType(),\n",
        "    \"status(vehicle_history)\": StringType(),\n",
        "    \"created_at(vehicle_history)\": TimestampType(),\n",
        "    \"updated_at(vehicle_history)\": TimestampType(),\n",
        "    \"deleted_at(vehicle_history)\": TimestampType(),\n",
        "    \"location_name\": StringType(),\n",
        "    \"created_at(location)\": TimestampType(),\n",
        "    \"updated_at(location)\": TimestampType(),\n",
        "    \"deleted_at(location)\": TimestampType()\n",
        "}\n",
        "\n",
        "# Iterate through the column type mapping and cast each column\n",
        "for column_name, data_type in column_type_mapping.items():\n",
        "    vehicles_location = vehicles_location.withColumn(\n",
        "        column_name, vehicles_location[column_name].cast(data_type)\n",
        "    )\n",
        "vehicles_location.show()\n"
      ],
      "metadata": {
        "id": "S_E5mXA2TU2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865e3f07-8292-4006-a040-1f500790608e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------+------------+--------------------+---------------+--------------------+-------------------+-------------------+-------------------+----------+-----------------------+---------------------------+---------------------------+---------------------------+---------------------------+-------------------------+----------------+--------+---------+--------------------+--------------------+--------------------+\n",
            "|location_id|vehicle_id|vehicle_name|license_plate_number|status(vehicle)|location_id(vehicle)|created_at(vehicle)|updated_at(vehicle)|deleted_at(vehicle)|history_id|status(vehicle_history)|created_at(vehicle_history)|updated_at(vehicle_history)|deleted_at(vehicle_history)|start_time(vehicle_history)|end_time(vehicle_history)|   location_name|latitude|longitude|created_at(location)|updated_at(location)|deleted_at(location)|\n",
            "+-----------+----------+------------+--------------------+---------------+--------------------+-------------------+-------------------+-------------------+----------+-----------------------+---------------------------+---------------------------+---------------------------+---------------------------+-------------------------+----------------+--------+---------+--------------------+--------------------+--------------------+\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10618|                 ONRENT|        2022-11-14 00:00:13|        2022-11-14 11:30:12|                       NULL|        2022-11-14 00:00:13|      2022-11-14 11:30:12|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10685|                OVERDUE|        2022-11-14 11:30:12|        2022-11-14 11:49:47|                       NULL|        2022-11-14 11:30:12|      2022-11-14 11:49:47|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10690|                   YARD|        2022-11-14 11:49:47|        2022-11-14 11:49:47|                       NULL|        2022-11-14 11:49:47|      2022-11-14 11:49:47|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10691|                PENDING|        2022-11-14 11:49:47|        2022-11-14 13:30:38|                       NULL|        2022-11-14 11:49:47|      2022-11-14 13:30:38|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10705|                   YARD|        2022-11-14 13:30:38|        2022-11-14 13:31:49|                       NULL|        2022-11-14 13:30:38|      2022-11-14 13:31:49|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10706|             RELOCATION|        2022-11-14 13:31:49|        2022-11-14 13:32:34|                       NULL|        2022-11-14 13:31:49|      2022-11-14 13:32:34|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10707|                   YARD|        2022-11-14 13:32:34|        2022-11-14 13:32:34|                       NULL|        2022-11-14 13:32:34|      2022-11-14 13:32:34|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10708|                PENDING|        2022-11-14 13:32:34|        2022-11-14 13:33:36|                       NULL|        2022-11-14 13:32:34|      2022-11-14 13:33:36|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "|          4|         2|Ford Everest|              1Q9243|           YARD|                   4|2022-09-04 23:37:56|2022-11-15 00:00:16|               NULL|     10709|                   YARD|        2022-11-14 13:33:36|        2022-11-15 00:00:16|                       NULL|        2022-11-14 13:33:36|      2022-11-15 00:00:16|Mandalay Airport|21.70487|95.968993| 2022-09-04 20:37:56| 2022-09-04 20:37:56|                NULL|\n",
            "+-----------+----------+------------+--------------------+---------------+--------------------+-------------------+-------------------+-------------------+----------+-----------------------+---------------------------+---------------------------+---------------------------+---------------------------+-------------------------+----------------+--------+---------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iii. Calculate daily utilization of each vehicle"
      ],
      "metadata": {
        "id": "dvJuoaRLS263"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define status and filter\n",
        "all_status = ['YARD', 'ONRENT', 'REPLACEMENT', 'DRIVE_CAR', 'CLEANING', 'RELOCATION', 'TRANSIT', 'PANELSHOP', 'MAINTENANCE', 'PENDING', 'OVERDUE']\n",
        "yard_status = ['YARD']\n",
        "all_status_df = vehicles_location.filter(vehicles_location['status(vehicle)'].isin(all_status))\n",
        "\n",
        "# Calculate duration column from start time and end time\n",
        "duration_df = all_status_df.withColumn(\n",
        "    \"duration\",\n",
        "    (unix_timestamp(col(\"end_time(vehicle_history)\")) - unix_timestamp(col(\"start_time(vehicle_history)\"))) /3600\n",
        ")\n",
        "# Group by date and sum of(duration, status) and calculate the total duration for each vehicle\n",
        "duration_by_vehicle = duration_df.groupBy([(to_date(\"created_at(vehicle_history)\")).alias('created_date'),\"vehicle_id\", \"vehicle_name\", \"license_plate_number\", \"location_name\"]) \\\n",
        "                                .agg(sum(\"duration\").alias(\"available_utilization_hours\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"YARD\", col(\"duration\"))).alias(\"idle_in_yard(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"ONRENT\", col(\"duration\"))).alias(\"on_rent(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"REPLACEMENT\", col(\"duration\"))).alias(\"replacement(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"DRIVE_CAR\", col(\"duration\"))).alias(\"drive_car(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"CLEANING\", col(\"duration\"))).alias(\"cleaning(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"RELOCATION\", col(\"duration\"))).alias(\"relocation(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"TRANSIT\", col(\"duration\"))).alias(\"transit(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"PANELSHOP\", col(\"duration\"))).alias(\"panelshop(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"MAINTENANCE\", col(\"duration\"))).alias(\"maintenance(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"PENDING\", col(\"duration\"))).alias(\"pending(hrs)\"),\n",
        "                                     sum(when(col(\"status(vehicle_history)\") == \"OVERDUE\", col(\"duration\"))).alias(\"overdue(hrs)\"),\n",
        "                                )\n",
        "\n",
        "# fill NULL data with 0\n",
        "duration_by_vehicle = duration_by_vehicle.fillna(0)\n",
        "\n",
        "# Round float data to 4 decimal places\n",
        "duration_by_vehicle = duration_by_vehicle.withColumn(\"available_utilization_hours\", round(col(\"available_utilization_hours\"), 4)) \\\n",
        "                                          .withColumn(\"idle_in_yard(hrs)\", round(col(\"idle_in_yard(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"on_rent(hrs)\", round(col(\"on_rent(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"replacement(hrs)\", round(col(\"replacement(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"drive_car(hrs)\", round(col(\"drive_car(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"cleaning(hrs)\", round(col(\"cleaning(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"relocation(hrs)\", round(col(\"relocation(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"transit(hrs)\", round(col(\"transit(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"panelshop(hrs)\", round(col(\"panelshop(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"maintenance(hrs)\", round(col(\"maintenance(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"pending(hrs)\", round(col(\"pending(hrs)\"), 4)) \\\n",
        "                                          .withColumn(\"overdue(hrs)\", round(col(\"overdue(hrs)\"), 4))\n",
        "\n",
        "# Calaculate the utilations based on duration status\n",
        "duration_by_vehicle = duration_by_vehicle.withColumn(\"utilization(%)\", (col(\"on_rent(hrs)\") + col(\"replacement(hrs)\") + col(\"overdue(hrs)\")) / col('available_utilization_hours') * 100) \\\n",
        "                                          .withColumn(\"drive_car_utilization(%)\", (col(\"drive_car(hrs)\")) / col('available_utilization_hours') * 100) \\\n",
        "                                          .withColumn(\"ops_utilization(%)\", (col(\"cleaning(hrs)\") + col(\"relocation(hrs)\") + col(\"transit(hrs)\") + col(\"panelshop(hrs)\") + col(\"maintenance(hrs)\") + col(\"pending(hrs)\")) / col('available_utilization_hours') * 100) \\\n",
        "                                          .withColumn(\"idle_utilization(%)\", col(\"idle_in_yard(hrs)\") / col('available_utilization_hours') * 100)\n",
        "\n",
        "# COnvert utilizations columns to 1 decimal place\n",
        "duration_by_vehicle = duration_by_vehicle.withColumn(\"utilization(%)\", format_number(col(\"utilization(%)\"), 0)) \\\n",
        "                                          .withColumn(\"drive_car_utilization(%)\", format_number(col(\"drive_car_utilization(%)\"), 0)) \\\n",
        "                                          .withColumn(\"ops_utilization(%)\", format_number(col(\"ops_utilization(%)\"), 0)) \\\n",
        "                                          .withColumn(\"idle_utilization(%)\", format_number(col(\"idle_utilization(%)\"), 0))\n",
        "\n",
        "\n",
        "duration_by_vehicle.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWr3vURVTBA4",
        "outputId": "cfc049d9-0ec8-424c-a4e9-301bbe1a2642"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+----------+------------+--------------------+----------------+---------------------------+-----------------+------------+----------------+--------------+-------------+---------------+------------+--------------+----------------+------------+------------+--------------+------------------------+------------------+-------------------+\n",
            "|created_date|vehicle_id|vehicle_name|license_plate_number|   location_name|available_utilization_hours|idle_in_yard(hrs)|on_rent(hrs)|replacement(hrs)|drive_car(hrs)|cleaning(hrs)|relocation(hrs)|transit(hrs)|panelshop(hrs)|maintenance(hrs)|pending(hrs)|overdue(hrs)|utilization(%)|drive_car_utilization(%)|ops_utilization(%)|idle_utilization(%)|\n",
            "+------------+----------+------------+--------------------+----------------+---------------------------+-----------------+------------+----------------+--------------+-------------+---------------+------------+--------------+----------------+------------+------------+--------------+------------------------+------------------+-------------------+\n",
            "|  2022-11-14|         2|Ford Everest|              1Q9243|Mandalay Airport|                    24.0008|          10.4642|     11.4997|             0.0|           0.0|          0.0|         0.0125|         0.0|           0.0|             0.0|      1.6981|      0.3264|            49|                       0|                 7|                 44|\n",
            "+------------+----------+------------+--------------------+----------------+---------------------------+-----------------+------------+----------------+--------------+-------------+---------------+------------+--------------+----------------+------------+------------+--------------+------------------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##iv. Load data to csv format"
      ],
      "metadata": {
        "id": "jRMBpwVZVGBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duration_by_vehicle.write.csv(\"DailyUtilizationOfEachVehicle.csv\", mode='Overwrite')"
      ],
      "metadata": {
        "id": "oGDY9VJA4B0t"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##v. To Schedule basics daily in Notebooks"
      ],
      "metadata": {
        "id": "hgZ2O547WCpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify functions to schedule\n",
        "def run_spark_job():\n",
        "  return duration_by_vehicle.write.parquet(\"DailyUtilizationOfEachVehicle.parquet\", mode='Overwrite')\n",
        "\n",
        "sched = BlockingScheduler()\n",
        "logging.info(\"Starting Spark job\")\n",
        "sched.add_job(run_spark_job, 'cron', hour=0, minute=0)\n",
        "#sched.start() # Uncomment to start schedule job\n",
        "logging.info(\"Spark job completed\")"
      ],
      "metadata": {
        "id": "NvSIFWEgmWF2"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}